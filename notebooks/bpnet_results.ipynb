{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BPNet LSTM Experiments\n\nThis notebook summarizes the experiments we ran for the BPNet project. We compare two implementations:\n\n- **LSTM Simple** \u2013 the original 2-layer, 128-hidden LSTM stack (no scheduler or weight decay).\n- **LSTM Tuned** \u2013 the larger 3-layer, 256-hidden LSTM with scheduling/regularization knobs.\n\nFor each model we train on different dataset sizes (20k vs. 50k segments, etc.) and visualize how the validation metrics evolve. The notebook automatically scans the `runs/` directory for training logs, so whenever we add a new run the tables and plots update automatically."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports & Utility Helpers"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8')\nROOT = Path('..').resolve()\nRUNS_ROOT = ROOT / 'runs'\nprint(f'Notebook root: {ROOT}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Load Metrics from All Runs\n\nEach training run writes a `metrics.csv` plus a `config.json`. We parse both to recover the model type, dataset slice, and per-epoch statistics."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def infer_dataset(train_mat: str) -> str:\n    name = train_mat.lower()\n    if '50k' in name:\n        return '50k segments'\n    if 'x10' in name or '200k' in name:\n        return '200k segments'\n    if 'train_subset' in name:\n        return 'full subset mat'\n    if 'oscar' in name:\n        return '20k segments'\n    return 'unknown'\n\n\ndef infer_model(log_dir: Path, config: dict) -> str:\n    log_name = str(log_dir).lower()\n    if 'simple' in log_name or not config:\n        return 'lstm_simple'\n    return 'lstm_tuned'\n\nrecords = []\nfor metrics_path in RUNS_ROOT.rglob('metrics.csv'):\n    log_dir = metrics_path.parent\n    config_path = log_dir / 'config.json'\n    config = {}\n    if config_path.exists():\n        with open(config_path) as f:\n            config = json.load(f)\n    df = pd.read_csv(metrics_path)\n    train_mat = config.get('train_mat', '')\n    dataset = infer_dataset(train_mat)\n    model_type = infer_model(log_dir, config)\n    for _, row in df.iterrows():\n        records.append({\n            'log_dir': str(log_dir.relative_to(ROOT)),\n            'model': model_type,\n            'dataset': dataset,\n            'epoch': int(row['epoch']),\n            'lr': row.get('lr'),\n            'train_loss': row['train_loss'],\n            'train_mae': row.get('train_mae'),\n            'train_rmse': row.get('train_rmse'),\n            'train_corr': row.get('train_corr'),\n            'val_loss': row.get('val_loss'),\n            'val_mae': row.get('val_mae'),\n            'val_rmse': row.get('val_rmse'),\n            'val_corr': row.get('val_corr'),\n        })\n\nmetrics_df = pd.DataFrame(records)\nif metrics_df.empty:\n    raise RuntimeError('No metrics.csv files found. Make sure training runs have logged metrics.')\n\nmetrics_df.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Best-epoch Summary\n\nWe aggregate the best validation loss and corresponding correlation for each (model, dataset, run) combination."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "summary = (metrics_df.dropna(subset=['val_loss'])\n                     .groupby(['log_dir', 'model', 'dataset'])\n                     .agg(best_val_loss=('val_loss', 'min'),\n                          best_val_corr=('val_corr', 'max'),\n                          epochs=('epoch', 'max'))\n                     .reset_index())\nsummary.sort_values('best_val_loss').reset_index(drop=True)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Validation Curves\n\nThe following plots compare how validation loss/correlation evolve for each configuration. Use the legend to distinguish runs and dataset sizes."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\nsns.lineplot(data=metrics_df, x='epoch', y='val_loss', hue='log_dir', style='dataset', ax=axes[0])\naxes[0].set_title('Validation Loss per Run')\naxes[0].set_ylabel('Val Loss (MSE)')\naxes[0].grid(alpha=0.3)\n\nsns.lineplot(data=metrics_df, x='epoch', y='val_corr', hue='log_dir', style='dataset', ax=axes[1])\naxes[1].set_title('Validation Correlation per Run')\naxes[1].set_ylabel('Pearson r')\naxes[1].grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Sample Predictions\n\nFor qualitative inspection we load the latest `epoch_*_samples.npz` stored inside a run directory and plot the first few target/pred pairs."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def preview_samples(log_dir: Path, max_pairs: int = 3):\n    sample_files = sorted(log_dir.glob('epoch_*_samples.npz'))\n    if not sample_files:\n        print(f'No sample files for {log_dir}')\n        return\n    data = np.load(sample_files[-1])\n    targets, preds = data['target_bp'], data['pred_bp']\n    pairs = min(max_pairs, targets.shape[0])\n    fig, axes = plt.subplots(pairs, 1, figsize=(10, 2 * pairs))\n    if pairs == 1:\n        axes = [axes]\n    t = np.arange(targets.shape[-1])\n    for i in range(pairs):\n        axes[i].plot(t, targets[i, 0], label='Target')\n        axes[i].plot(t, preds[i, 0], label='Predicted', alpha=0.8)\n        axes[i].set_title(f'Segment {i+1}')\n        axes[i].set_xlabel('Sample idx')\n        axes[i].set_ylabel('BP (norm)')\n        axes[i].grid(alpha=0.3)\n    axes[0].legend()\n    plt.tight_layout()\n    plt.show()\n\nbest_log = summary.sort_values('best_val_loss').iloc[0]['log_dir']\nprint(f'Previewing samples for {best_log}')\npreview_samples(ROOT / best_log)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Architecture Reference\n\n| Model | Conv Layers | LSTM Hidden Size / Layers | Scheduler & Weight Decay |\n|-------|-------------|---------------------------|---------------------------|\n| `lstm_simple` | 2 conv layers (32 filters, kernel 7) | 128 hidden units \u00d7 2 layers, dropout 0.1 | None |\n| `lstm_tuned`  | Same conv front-end | 256 hidden units \u00d7 3 layers, dropout 0.2 | Adam + weight decay 1e-4 + ReduceLROnPlateau |\n\nBoth models share the same per-segment normalization and MSE targets. The tuned variant simply adds capacity and better optimization controls, while the simple variant is the baseline used earlier in the project."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Takeaways\n\n- The tuned LSTM consistently reaches lower validation loss/correlation when trained on 20k segments, but it needs the scheduler/regularization to behave.\n- Scaling the dataset (e.g., 50k+) shifts the curves downward for both models, showing the benefit of exposing the LSTM to more subjects.\n- The sample visualizations remain useful for sanity-checking: look for systematic phase shifts or amplitude bias when you iterate on preprocessing.\n\nFeel free to rerun training with different dataset slices; as soon as the `runs/` directory contains their logs, this notebook will update the tables/plots automatically."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}