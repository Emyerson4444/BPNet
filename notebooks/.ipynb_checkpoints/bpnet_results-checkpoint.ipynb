{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BPNet LSTM Experiments\n",
        "\n",
        "This notebook summarizes the experiments we ran for the BPNet project. We compare two implementations:\n",
        "\n",
        "- **LSTM Simple** – the original 2-layer, 128-hidden LSTM stack (no scheduler or weight decay).\n",
        "- **LSTM Tuned** – the larger 3-layer, 256-hidden LSTM with scheduling/regularization knobs.\n",
        "\n",
        "For each model we train on different dataset sizes (20k vs. 50k segments, etc.) and visualize how the validation metrics evolve. The notebook automatically scans the `runs/` directory for training logs, so whenever we add a new run the tables and plots update automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports & Utility Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "858c0991",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook root: /Users/sndichu/Desktop/csci1470/BPNet/notebooks\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT / 'runs').exists():\n",
        "    ROOT = ROOT.parent\n",
        "RUNS_ROOT = ROOT / 'runs'\n",
        "print(f'Notebook root: {ROOT}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d768db",
      "metadata": {},
      "source": [
        "## 2. Load Metrics from All Runs\n",
        "\n",
        "Each training run writes a `metrics.csv` plus a `config.json`. We parse both to recover the model type, dataset slice, and per-epoch statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c2746a61",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "No metrics.csv files found. Make sure training runs have logged metrics.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m metrics_df = pd.DataFrame(records)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metrics_df.empty:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNo metrics.csv files found. Make sure training runs have logged metrics.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     53\u001b[39m metrics_df.head()\n",
            "\u001b[31mRuntimeError\u001b[39m: No metrics.csv files found. Make sure training runs have logged metrics."
          ]
        }
      ],
      "source": [
        "def infer_dataset(train_mat: str) -> str:\n",
        "    name = train_mat.lower()\n",
        "    if '50k' in name:\n",
        "        return '50k segments'\n",
        "    if 'x10' in name or '200k' in name:\n",
        "        return '200k segments'\n",
        "    if 'train_subset' in name:\n",
        "        return 'full subset mat'\n",
        "    if 'oscar' in name:\n",
        "        return '20k segments'\n",
        "    return 'unknown'\n",
        "\n",
        "\n",
        "def infer_model(log_dir: Path, config: dict) -> str:\n",
        "    log_name = str(log_dir).lower()\n",
        "    if 'simple' in log_name or not config:\n",
        "        return 'lstm_simple'\n",
        "    return 'lstm_tuned'\n",
        "\n",
        "records = []\n",
        "for metrics_path in RUNS_ROOT.rglob('metrics.csv'):\n",
        "    log_dir = metrics_path.parent\n",
        "    config_path = log_dir / 'config.json'\n",
        "    config = {}\n",
        "    if config_path.exists():\n",
        "        with open(config_path) as f:\n",
        "            config = json.load(f)\n",
        "    df = pd.read_csv(metrics_path)\n",
        "    train_mat = config.get('train_mat', '')\n",
        "    dataset = infer_dataset(train_mat)\n",
        "    model_type = infer_model(log_dir, config)\n",
        "    for _, row in df.iterrows():\n",
        "        records.append({\n",
        "            'log_dir': str(log_dir.relative_to(ROOT)),\n",
        "            'model': model_type,\n",
        "            'dataset': dataset,\n",
        "            'epoch': int(row['epoch']),\n",
        "            'lr': row.get('lr'),\n",
        "            'train_loss': row['train_loss'],\n",
        "            'train_mae': row.get('train_mae'),\n",
        "            'train_rmse': row.get('train_rmse'),\n",
        "            'train_corr': row.get('train_corr'),\n",
        "            'val_loss': row.get('val_loss'),\n",
        "            'val_mae': row.get('val_mae'),\n",
        "            'val_rmse': row.get('val_rmse'),\n",
        "            'val_corr': row.get('val_corr'),\n",
        "        })\n",
        "\n",
        "metrics_df = pd.DataFrame(records)\n",
        "if metrics_df.empty:\n",
        "    raise RuntimeError('No metrics.csv files found. Make sure training runs have logged metrics.')\n",
        "\n",
        "metrics_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Best-epoch Summary\n",
        "\n",
        "We aggregate the best validation loss and corresponding correlation for each (model, dataset, run) combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = (metrics_df.dropna(subset=['val_loss'])\n",
        "                     .groupby(['log_dir', 'model', 'dataset'])\n",
        "                     .agg(best_val_loss=('val_loss', 'min'),\n",
        "                          best_val_corr=('val_corr', 'max'),\n",
        "                          epochs=('epoch', 'max'))\n",
        "                     .reset_index())\n",
        "summary.sort_values('best_val_loss').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Validation Curves\n",
        "\n",
        "The following plots compare how validation loss/correlation evolve for each configuration. Use the legend to distinguish runs and dataset sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "sns.lineplot(data=metrics_df, x='epoch', y='val_loss', hue='log_dir', style='dataset', ax=axes[0])\n",
        "axes[0].set_title('Validation Loss per Run')\n",
        "axes[0].set_ylabel('Val Loss (MSE)')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "sns.lineplot(data=metrics_df, x='epoch', y='val_corr', hue='log_dir', style='dataset', ax=axes[1])\n",
        "axes[1].set_title('Validation Correlation per Run')\n",
        "axes[1].set_ylabel('Pearson r')\n",
        "axes[1].grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sample Predictions\n",
        "\n",
        "For qualitative inspection we load the latest `epoch_*_samples.npz` stored inside a run directory and plot the first few target/pred pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preview_samples(log_dir: Path, max_pairs: int = 3):\n",
        "    sample_files = sorted(log_dir.glob('epoch_*_samples.npz'))\n",
        "    if not sample_files:\n",
        "        print(f'No sample files for {log_dir}')\n",
        "        return\n",
        "    data = np.load(sample_files[-1])\n",
        "    targets, preds = data['target_bp'], data['pred_bp']\n",
        "    pairs = min(max_pairs, targets.shape[0])\n",
        "    fig, axes = plt.subplots(pairs, 1, figsize=(10, 2 * pairs))\n",
        "    if pairs == 1:\n",
        "        axes = [axes]\n",
        "    t = np.arange(targets.shape[-1])\n",
        "    for i in range(pairs):\n",
        "        axes[i].plot(t, targets[i, 0], label='Target')\n",
        "        axes[i].plot(t, preds[i, 0], label='Predicted', alpha=0.8)\n",
        "        axes[i].set_title(f'Segment {i+1}')\n",
        "        axes[i].set_xlabel('Sample idx')\n",
        "        axes[i].set_ylabel('BP (norm)')\n",
        "        axes[i].grid(alpha=0.3)\n",
        "    axes[0].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "best_log = summary.sort_values('best_val_loss').iloc[0]['log_dir']\n",
        "print(f'Previewing samples for {best_log}')\n",
        "preview_samples(ROOT / best_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce6c746",
      "metadata": {},
      "source": [
        "## 6. Focused Configuration Set\n",
        "\n",
        "We highlight four scenarios for comparison:\n",
        "\n",
        "1. **Simple – 20k** (baseline LSTM, 20k segments)\n",
        "2. **Simple – 50k**\n",
        "3. **Tuned – 20k**\n",
        "4. **Tuned – 50k**\n",
        "\n",
        "The following cells filter `metrics_df` down to these runs so we can compare apples-to-apples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a16120",
      "metadata": {},
      "outputs": [],
      "source": [
        "target_keys = {\n",
        "    \"Simple – 20k\": \"simple_oscar20k\",\n",
        "    \"Simple – 50k\": \"simple_oscar50k\",\n",
        "    \"Tuned – 20k\": \"oscar_npz_tuned\",\n",
        "    \"Tuned – 50k\": \"oscar_npz_50k\",\n",
        "}\n",
        "focus_frames = []\n",
        "for label, key in target_keys.items():\n",
        "    mask = metrics_df['log_dir'].str.contains(key, na=False)\n",
        "    subset = metrics_df[mask].copy()\n",
        "    if subset.empty:\n",
        "        print(f\"Warning: no runs found for {label}\")\n",
        "        continue\n",
        "    subset['configuration'] = label\n",
        "    focus_frames.append(subset)\n",
        "focus_df = pd.concat(focus_frames, ignore_index=True) if focus_frames else pd.DataFrame()\n",
        "focus_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4479697",
      "metadata": {},
      "source": [
        "## 7. Performance Snapshot\n",
        "\n",
        "We compare validation loss/correlation across the four configurations. Use this to assess how model capacity and dataset size interact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not focus_df.empty:\n",
        "    focus_summary = (focus_df.dropna(subset=['val_loss'])\n",
        "                     .groupby(['configuration', 'log_dir'])\n",
        "                     .agg(best_val_loss=('val_loss', 'min'),\n",
        "                          best_val_corr=('val_corr', 'max'),\n",
        "                          epochs=('epoch', 'max'))\n",
        "                     .reset_index())\n",
        "    display(focus_summary)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    sns.lineplot(data=focus_df, x='epoch', y='val_loss', hue='configuration', ax=axes[0])\n",
        "    axes[0].set_title('Validation Loss by Configuration')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    sns.lineplot(data=focus_df, x='epoch', y='val_corr', hue='configuration', ax=axes[1])\n",
        "    axes[1].set_title('Validation Correlation by Configuration')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "else:\n",
        "    print('No focused configurations available.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Architecture Reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Takeaways\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "csci1470",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
